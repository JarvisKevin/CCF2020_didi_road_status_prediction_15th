{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonLocalBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 inter_channels,\n",
    "                 out_channels,\n",
    "                 dimension=1,\n",
    "                 sub_sample=True,          \n",
    "                 bn_layer=True):\n",
    "        super(NonLocalBlock, self).__init__()\n",
    "        \n",
    "        # 确定维度\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # 三个 1*1\n",
    "        self.g = nn.Conv1d(in_channels=self.in_channels,\n",
    "                 out_channels=self.inter_channels,\n",
    "                 kernel_size=1,\n",
    "                 stride=1,\n",
    "                 padding=0)       \n",
    "        \n",
    "        self.theta = nn.Conv1d(in_channels=self.in_channels,\n",
    "                             out_channels=self.inter_channels,\n",
    "                             kernel_size=1,\n",
    "                             stride=1,\n",
    "                             padding=0)\n",
    "        self.phi = nn.Conv1d(in_channels=self.in_channels,\n",
    "                           out_channels=self.inter_channels,\n",
    "                           kernel_size=1,\n",
    "                           stride=1,\n",
    "                           padding=0)\n",
    "        \n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, nn.MaxPool1d(kernel_size=(2)))\n",
    "            self.phi = nn.Sequential(self.phi, nn.MaxPool1d(kernel_size=(2)))\n",
    "        \n",
    "        # 最后的 1*1\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                nn.Conv1d(in_channels=self.inter_channels,\n",
    "                          out_channels=self.out_channels,\n",
    "                          kernel_size=1,\n",
    "                          stride=1,\n",
    "                          padding=0), \n",
    "                nn.BatchNorm1d(self.in_channels))\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = nn.Conv1d(in_channels=self.inter_channels,\n",
    "                             out_channels=self.out_channels,\n",
    "                             kernel_size=1,\n",
    "                             stride=1,\n",
    "                             padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (B, F, N)\n",
    "        :return:\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)         #[bs, c, N]\n",
    "        g_x = g_x.permute(0, 2, 1)                                        #[bs, N, c]\n",
    "        \n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1) #[bs, c, N]\n",
    "        theta_x = theta_x.permute(0, 2, 1)                                #[bs, N, c]\n",
    "        \n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)     #[bs, c, N] \n",
    "        \n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x                                                       \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Road_status(nn.Module):\n",
    "    def __init__(self, embedding_pretrained_dict=embedding_dict):\n",
    "        super(Road_status, self).__init__()\n",
    "        \n",
    "        # 数据一：历史与实时路况\n",
    "        # 近期路况特征\n",
    "        # 20 --> 4\n",
    "        self.curr_t = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0, dilation=4), # T  20-->4   \n",
    "            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1, dilation=1))# 4-->8*4\n",
    "        # 20 --> 5\n",
    "        self.curr_c = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=16, kernel_size=4, stride=4, padding=0, dilation=1), # T  20-->5   \n",
    "            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1, dilation=1))# 4-->8*5\n",
    "        # Non_Local\n",
    "        self.cur_nonlocalblock = NonLocalBlock(in_channels=8, inter_channels=16, out_channels=8, sub_sample=False, bn_layer=False)                                                 \n",
    "        \n",
    "        # 历史路况特征\n",
    "        self.hist = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=8, dilation=4), nn.ReLU(),       # 100\n",
    "            nn.Conv1d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=8, dilation=4), nn.ReLU(),      # 100\n",
    "            nn.Conv1d(in_channels=64, out_channels=32, kernel_size=4, stride=4, padding=0, dilation=1), nn.ReLU(),    # 25 \n",
    "            nn.Conv1d(in_channels=32, out_channels=8, kernel_size=5, stride=5, padding=0, dilation=1), nn.ReLU())     # 8*5\n",
    "        \n",
    "        \n",
    "        # 数据二：道路属性\n",
    "        # link_id embedding，预训练来自GCN\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_dict, freeze=False)\n",
    "        # 其他道路特征\n",
    "        self.attr = nn.Sequential(\n",
    "            nn.Linear(in_features=14, out_features=32), nn.ReLU(),\n",
    "            nn.Linear(in_features=32, out_features=32), nn.ReLU()) \n",
    "\n",
    "        # 全连接\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(156, 64),\n",
    "                nn.Linear(64, 16), nn.ReLU(),\n",
    "                nn.Linear(16, 3))\n",
    "        \n",
    "        self.flatten = nn.Flatten()    \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''x：(B,112)'''    \n",
    "        link = self.embed(x[:,0].long())\n",
    "        attr = self.attr(x[:,1:15])\n",
    "        curr_t = self.curr_t(x[:,-20:].unsqueeze(1)) # 8*4\n",
    "        curr_c = self.curr_c(x[:,-20:].unsqueeze(1)) # 8*5       \n",
    "        cur = torch.cat([curr_t,curr_c], dim=-1)        \n",
    "        cur = self.cur_nonlocalblock(cur)\n",
    "        cur = self.flatten(cur)        \n",
    "        \n",
    "        hist = self.hist(x[:,15:-20].unsqueeze(1))   # 8*5\n",
    "        hist = self.flatten(hist)\n",
    "        \n",
    "        feat = torch.cat([link, attr, cur, hist], dim=-1)\n",
    "        y = self.fc(feat)\n",
    "        out = self.softmax(y)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
